{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape data from pushshift API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"reddit\"\n",
    "PUSHSHIFT_REDDIT_URL = \"http://api.pushshift.io/reddit\"\n",
    "MONGO_URI = \"mongodb://35.230.135.125:27017/\"\n",
    "MONGO_DB = \"reddit\"\n",
    "MONGO_SUBS_COLLECTION = \"submissions\"\n",
    "SUBS_FILE = \"submission.json\"\n",
    "COMMENTS_FILE = \"comment.json\"\n",
    "CLEAN_COMMENTS_FILE = \"clean_comments.json\"\n",
    "COMMENT_EMOTIONS_FILE = \"comment_emotions.json\"\n",
    "COMMENT_EMOTION_THRESHOLD = 0.2\n",
    "DEFAULT_THRESHOLDS = [0.5] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def fetchObjects(data_type=\"submission\", subreddit=\"Art\", after=1104537600):\n",
    "    params = {\n",
    "        \"sort_type\":\"created_utc\",\n",
    "        \"sort\":\"asc\",\n",
    "        \"size\": 1000,\n",
    "        \"type\": data_type,\n",
    "        \"subreddit\": subreddit,\n",
    "        \"after\": after\n",
    "    }\n",
    "\n",
    "    r = requests.get(PUSHSHIFT_REDDIT_URL + \"/\" + data_type + \"/search/\", params=params, timeout=30)\n",
    "    if r.status_code == 200:\n",
    "        response = json.loads(r.text)\n",
    "        data = response['data']\n",
    "        return data\n",
    "\n",
    "def scrape_reddit(data_type=\"submission\", subreddit=\"Art\"):\n",
    "    try:\n",
    "        file = open(f\"{ROOT_DIR}/{data_type}.json\", \"r+\")\n",
    "        for line in file:\n",
    "            pass\n",
    "        last_item = json.loads(line)\n",
    "        after = last_item.get(\"created_utc\", 1104537600)\n",
    "    except:\n",
    "        file = open(f\"{ROOT_DIR}/{data_type}.json\", \"w\")\n",
    "        after = 1104537600\n",
    "        \n",
    "    while True:\n",
    "        new_data = fetchObjects(data_type, subreddit, after) \n",
    "        for item in new_data:\n",
    "            file.write(json.dumps(item) + \"\\n\")\n",
    "        after = item[\"created_utc\"]\n",
    "        dt_object = datetime.fromtimestamp(after)\n",
    "        print(\"after =\", dt_object)\n",
    "        print(\"results =\", len(new_data))\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "\n",
    "def get_collection(mongo_uri=MONGO_URI, db=MONGO_DB, col=MONGO_SUBS_COLLECTION):\n",
    "    myclient = pymongo.MongoClient(mongo_uri)\n",
    "    mydb = myclient[db]\n",
    "    collection = mydb[col]\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import pymongo\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def build_mongo_submissions_collection(filename=SUBS_FILE, db=MONGO_DB, collection=MONGO_SUBS_COLLECTION, mongo_uri=MONGO_URI):\n",
    "    subs = get_collection(mongo_uri, db, collection)\n",
    "    subs.create_index(\"id\", unique=True)\n",
    "    subs.create_index(\"downloadTried\")\n",
    "    subs.create_index(\"local_image_path\")\n",
    "    \n",
    "    with jsonlines.open(f\"{ROOT_DIR}/{filename}\") as reader:\n",
    "        for obj in tqdm(reader):\n",
    "            try:\n",
    "                subs.insert(obj)\n",
    "            except KeyboardInterrupt: \n",
    "                break\n",
    "            except: \n",
    "                continue\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15ec777dc754ab48b39ddd98d786a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stathis/.local/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# build_mongo_submissions_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the comments for emotion classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"reddit\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import  collect_list, struct, col, udf, lit, concat, lower\n",
    "from pyspark.sql.types import StructType, StringType, ArrayType, FloatType, BooleanType, IntegerType\n",
    "\n",
    "\n",
    "emot_schema = StructType()\\\n",
    "    .add(\"id\", StringType(), True)\\\n",
    "    .add(\"emotion_vec\", ArrayType(FloatType()), True)\n",
    "\n",
    "\n",
    "com_schema = StructType()\\\n",
    "    .add(\"id\", StringType(), True)\\\n",
    "    .add(\"author\", StringType(), True)\\\n",
    "    .add(\"link_id\", StringType(), True)\\\n",
    "    .add(\"parent_id\", StringType(), True)\\\n",
    "    .add(\"body\", StringType(), True)\\\n",
    "    .add(\"subreddit\", StringType(), True)\\\n",
    "    .add(\"created_utc\", StringType(), True)\\\n",
    "    .add(\"score\", StringType(), True)\n",
    "\n",
    "\n",
    "def prepare_comments_df(path=COMMENTS_FILE):\n",
    "    \"\"\"\n",
    "        Loads scraped comments file into a spark dataframe.\n",
    "        Filters out main moderator comments and deleted commments.\n",
    "    \"\"\"\n",
    "    df = spark.read.schema(com_schema).json(f\"{ROOT_DIR}/{path}\")\n",
    "    df = df.filter(col(\"link_id\") == col(\"parent_id\"))\n",
    "    df = df.filter(col(\"author\") != \"AutoModerator\")\n",
    "    df = df.filter(col(\"author\") != \"art_moderator_bot\")\n",
    "    df = df.filter(col(\"body\") != \"[deleted]\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def df_save_json(df, filename=CLEAN_COMMENTS_FILE):\n",
    "    df.repartition(1).write.json(f\"{ROOT_DIR}/{filename}_dir\")\n",
    "    os.system(f\"cat {ROOT_DIR}/{filename}_dir/*.json > {ROOT_DIR}/{filename}\")\n",
    "    os.system(f\"rm -rf {ROOT_DIR}/{filename}_dir\")\n",
    "    \n",
    "    \n",
    "def task_create_clean_comments_file(path=COMMENTS_FILE):\n",
    "    df = prepare_comments_df(path)\n",
    "    df.show()\n",
    "    df_save_json(df, CLEAN_COMMENTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+---------+--------------------+---------+-----------+-----+\n",
      "|     id|      author| link_id|parent_id|                body|subreddit|created_utc|score|\n",
      "+-------+------------+--------+---------+--------------------+---------+-----------+-----+\n",
      "|c03furo|    nerdlife|t3_6a2bf| t3_6a2bf|Im not going to d...|      Art| 1205457635|    2|\n",
      "|c03h794|      OMouse|t3_6cjdw| t3_6cjdw|Post your art som...|      Art| 1205865252|    3|\n",
      "|c03hd6h|dsg123456789|t3_6cjdw| t3_6cjdw|It would help if ...|      Art| 1205899588|    1|\n",
      "|c03hjzh|      OMouse|t3_6copr| t3_6copr|What's really nic...|      Art| 1205954100|    1|\n",
      "|c03hkdv|   kickstand|t3_6cosf| t3_6cosf|I knew a Lucia on...|      Art| 1205956082|    2|\n",
      "|c03ij9g|      OMouse|t3_6d1ql| t3_6d1ql|We need more subm...|      Art| 1206236946|    2|\n",
      "|c03imvr|   [deleted]|t3_6d30x| t3_6d30x|**copy, paste all...|      Art| 1206285242|    1|\n",
      "|c03iu7e|    nerdlife|t3_6d1wp| t3_6d1wp|Yeh, Andy Kehoe r...|      Art| 1206347403|    1|\n",
      "|c03iw7i|      OMouse|t3_6d2xv| t3_6d2xv|I find this type ...|      Art| 1206370028|    2|\n",
      "|c03izrd|   [deleted]|t3_6d1wp| t3_6d1wp|This is amazing u...|      Art| 1206388540|    1|\n",
      "|c03izs2|   [deleted]|t3_6d1ql| t3_6d1ql|          I'm on it.|      Art| 1206388624|    1|\n",
      "|c03j9w2|   [deleted]|t3_6dbwk| t3_6dbwk|I think this is b...|      Art| 1206464516|    1|\n",
      "|c03jkuj|   [deleted]|t3_6d2rc| t3_6d2rc|Hell yes. Bookmar...|      Art| 1206538156|    2|\n",
      "|c03jmlt|   [deleted]|t3_6dgm1| t3_6dgm1|I'm Bud Clay Thom...|      Art| 1206546926|    2|\n",
      "|c03jmqe|   [deleted]|t3_6dgm1| t3_6dgm1|I am a fabric and...|      Art| 1206547459|    2|\n",
      "|c03jmwz|   [deleted]|t3_6dgm1| t3_6dgm1|I'm a self employ...|      Art| 1206548161|    3|\n",
      "|c03jn0q|    moultano|t3_6dgm1| t3_6dgm1|I currently get a...|      Art| 1206548494|    3|\n",
      "|c03jn3v|nightbiscuit|t3_6dgm1| t3_6dgm1|some of my painti...|      Art| 1206548856|    7|\n",
      "|c03jnaa|     DTanner|t3_6dgm1| t3_6dgm1|I wouldn't really...|      Art| 1206549491|    2|\n",
      "|c03jnab|       qgyh2|t3_6dgm1| t3_6dgm1|I make miniature ...|      Art| 1206549495|    0|\n",
      "+-------+------------+--------+---------+--------------------+---------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task_create_clean_comments_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train comment emotion detection model\n",
    "\n",
    "Based on this data https://github.com/sarnthil/unify-emotion-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_emotions = [\n",
    "    (\"joy\", \"sadness\"),\n",
    "    (\"anger\", \"fear\"),\n",
    "    (\"trust\", \"disgust\"),\n",
    "    (\"surprise\", \"anticipation\")\n",
    "]\n",
    "\n",
    "def data():\n",
    "  with smart_open.smart_open(\"https://storage.googleapis.com/pushshift_reddit/unified-dataset.jsonl\") as reader:\n",
    "    for obj in tqdm(reader, total=221439):\n",
    "      d = json.loads(obj)\n",
    "      text = d['text']\n",
    "      emotions = d['emotions']\n",
    "      labels = []\n",
    "      for p, n in all_emotions:\n",
    "        if emotions.get(p, 0):\n",
    "          l = 1.0\n",
    "        elif emotions.get(n, 0):\n",
    "          l = -1.0\n",
    "        else:\n",
    "          l = 0.0\n",
    "        labels.append(l)\n",
    "      yield text, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate comment emotions to get submission vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def threshold(emotion_vec, t):\n",
    "    res = np.asarray(emotion_vec).astype(np.float32) -  np.asarray(t).astype(np.float32)\n",
    "    \n",
    "#     res = np.where(res > 0, res, 0)\n",
    "#     res = np.argmax(res)\n",
    "    \n",
    "    res = np.where(res > 0, 1, 0)\n",
    "#     res = np.argmax(res)\n",
    "    \n",
    "    return res.tolist()\n",
    "\n",
    "\n",
    "def load_comment_emotions(path=COMMENT_EMOTIONS_FILE, thresholds=DEFAULT_THRESHOLDS):\n",
    "    \"\"\"\n",
    "        Loads emotion vectors predicted by the ktrain bert model on comments.\n",
    "    \"\"\"\n",
    "    threshold_udf = udf(lambda z: threshold(z, t), ArrayType(IntegerType()))\n",
    "    df = spark.read.schema(emot_schema).json(f\"{ROOT_DIR}/{path}\")\n",
    "    df = df.withColumn(\"class\" , threshold_udf(df.emotion_vec))\n",
    "    return df\n",
    "\n",
    "\n",
    "def aggregate_emotions(emotions):\n",
    "    weights = np.asarray([w for vec, w in emotions if int(w) > 0]).astype(np.float32)\n",
    "    vectors = np.asarray([vec for vec, w in emotions if int(w) > 0]).astype(np.float32)\n",
    "    try: \n",
    "        return np.average(vectors, weights=weights, axis=0).tolist()\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "  \n",
    "\n",
    "def aggregate_emotion_vectors(comments_df, emotions_df):\n",
    "    \"\"\"\n",
    "        Create submission emotion vectors by doing a weighted average of comment\n",
    "        vectors using as weight the comment score.\n",
    "    \"\"\"\n",
    "    aggregate_emotions_udf = udf(lambda z: aggregate_emotions(z), ArrayType(FloatType()))\n",
    "    df = comments_df.join(emotions_df, emotions_df.id == comments_df.id , \"inner\")\n",
    "    df = df.groupby(\"link_id\").agg(collect_list(struct(\"emotion_vec\", \"score\" )).alias('emotion_vectors'))\n",
    "    df = df.withColumn(\"aggregated_emotion\", aggregate_emotions_udf(\"emotion_vectors\"))\n",
    "    df = df.select(\"link_id\", \"aggregated_emotion\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def update_mongo_submission_emotions(aggregated_emotions_df, mongo_uri, db, col):\n",
    "    collection = get_collection(mongo_uri, db, col)\n",
    "    data = aggregated_emotions_df.toPandas().to_dict(orient='records')\n",
    "    for sub in tqdm(data):\n",
    "        _id = sub[\"link_id\"].replace(\"t3_\", \"\")\n",
    "        emotion = sub[\"aggregated_emotion\"]\n",
    "        collection.update({ \"id\": _id}, { \"$set\": { \"emotion_vec\": emotion}})\n",
    "\n",
    "\n",
    "def task_aggregate_emotions(comments_path, comment_emotions_path, thresholds, mongo_uri, db, collection):\n",
    "    comments_df =  prepare_comments_df(comments_path)\n",
    "    emotions_df =  load_comment_emotions(comment_emotions_path, thresholds=thresholds)\n",
    "    aggregated_emotions_df = aggregate_emotion_vectors(comments_df, emotions_df)\n",
    "    aggregated_emotions_df.show()\n",
    "    update_mongo_submission_emotions(aggregated_emotions_df, mongo_uri, db, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "    \"joy\": 0.18,\n",
    "    \"sadness\": 0.09,\n",
    "    \"anger\": 0.04,\n",
    "    \"fear\": 0.09,\n",
    "    \"trust\": 0.02,\n",
    "    \"disgust\": 0.02,\n",
    "    \"surprise\": 0.04,\n",
    "    \"noemo\": 0.43,\n",
    "    \"other\": 0.01,\n",
    "    \"missing\": 0.08\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|  link_id|  aggregated_emotion|\n",
      "+---------+--------------------+\n",
      "|t3_102aax|[0.20011702, 0.05...|\n",
      "|t3_10gub6|[0.12210401, 0.05...|\n",
      "|t3_10qhd2|[0.43869275, 0.02...|\n",
      "|t3_10ty9j|[0.04781838, 0.00...|\n",
      "|t3_10y64t|[0.005038334, 0.1...|\n",
      "|t3_111zxi|[0.19062994, 0.05...|\n",
      "|t3_11230w|[0.35857823, 0.01...|\n",
      "|t3_11are5|[0.31641218, 0.01...|\n",
      "|t3_11odq4|[0.062355034, 0.0...|\n",
      "|t3_1218z6|[0.01670531, 2.56...|\n",
      "|t3_121age|[8.4310834E-4, 2....|\n",
      "|t3_128d69|[0.015222427, 0.0...|\n",
      "|t3_12j87i|[0.31269696, 0.00...|\n",
      "|t3_12ru2t|[0.31035754, 0.01...|\n",
      "|t3_12rxlk|[0.7862867, 0.002...|\n",
      "|t3_132oct|[0.0252282, 0.019...|\n",
      "|t3_136902|[0.48327294, 0.01...|\n",
      "|t3_13o6su|[0.66143954, 0.32...|\n",
      "|t3_13s2ce|[0.20395271, 0.01...|\n",
      "|t3_145iy8|[0.02086342, 0.10...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20e06439e5a4cf48cbd00bfe55c84df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=423506.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stathis/.local/lib/python3.7/site-packages/ipykernel_launcher.py:55: DeprecationWarning: update is deprecated. Use replace_one, update_one or update_many instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "task_aggregate_emotions(CLEAN_COMMENTS_FILE, COMMENT_EMOTIONS_FILE, t, MONGO_URI, MONGO_DB, MONGO_SUBS_COLLECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fire\n",
    "import pymongo\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_image(_id, url, local_path ,timeout=5):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=timeout)\n",
    "        if r.status_code in range(200, 209):\n",
    "            extension = url.split(\".\")[-1]\n",
    "            path = f\"{local_path}/{_id}.{extension}\"\n",
    "            with open(path, 'wb') as f:\n",
    "                f.write(r.content) \n",
    "            r.close()\n",
    "            return path\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def work(collection, local_path):\n",
    "    sample = collection.find_one_and_update(\n",
    "        {\"downloadTried\": { \"$exists\": False }},\n",
    "        {\"$set\": { \"downloadTried\": True }})\n",
    "    if sample:\n",
    "        url = sample[\"url\"]\n",
    "        _id = sample[\"id\"]\n",
    "        path = download_image(_id, url, local_path)\n",
    "        if path:\n",
    "            collection.update_one(\n",
    "                { \"id\": _id },\n",
    "                {\"$set\": { \"local_image_path\": path, \"image_was_downloaded\": True }}\n",
    "            )\n",
    "        print(_id, url, path)\n",
    "\n",
    "\n",
    "def run(db=\"reddit\", col=\"submissions\", local_path=\"reddit/mongo_images\", mongo_uri=\"mongodb://35.230.135.125:27017/\"):\n",
    "    collection = get_collection(mongo_uri, db, col)\n",
    "\n",
    "    while True:\n",
    "        work(collection, local_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess images and save as numpy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "def get_images_and_emotions_df(threshold ,mongo_uri=MONGO_URI, db=MONGO_DB, col=MONGO_SUBS_COLLECTION):\n",
    "    collection = get_collection(mongo_uri, db, col)\n",
    "    data = collection.find(\n",
    "        { \"emotion_vec\": {\"$exists\": True }, \"local_image_path\": {\"$exists\": True }, \"image_was_downloaded\": True }, \n",
    "        { \"local_image_path\": 1, \"emotion_vec\": 1 })\n",
    "    data = (d for d in data if d[\"emotion_vec\"])\n",
    "    data = (d for d in data if max(d[\"emotion_vec\"]) > threshold)\n",
    "    df =  pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def plot_images(img_gen, batch_size=32, n=4):\n",
    "    m = int(batch_size / n)\n",
    "    fig, ax = plt.subplots(m, n, figsize=(10, 2*m))\n",
    "    plt.setp(ax, xticks=[], yticks=[])\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    for (imgs, labels) in img_gen:\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                if i*n + j < batch_size:\n",
    "                    ax[i][j].imshow(imgs[i*n + j])\n",
    "        break\n",
    "\n",
    "def fixed_datagen(datagen, batches=452):\n",
    "    for i in range(batches):\n",
    "        try:\n",
    "            x, y = next(datagen)\n",
    "            fixed_y = np.vstack(y)\n",
    "            yield x, fixed_y\n",
    "        except (KeyboardInterrupt, StopIteration) as e:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        \n",
    "def generate_numpy_dataset(df, save_dir):\n",
    "    imgen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    datagen = imgen.flow_from_dataframe(\n",
    "        df, \n",
    "        shuffle=False,\n",
    "        x_col=\"local_image_path\", \n",
    "        y_col=\"emotion_vec\", \n",
    "        class_mode=\"raw\", \n",
    "        batch_size=256,\n",
    "        validate_filenames=False)\n",
    "    n_batches = int(df[\"local_image_path\"].size / 256)\n",
    "    batches = fixed_datagen(datagen, batches=n_batches)\n",
    "    for i, (x, y) in enumerate(tqdm(batches, total=n_batches)):\n",
    "        batch_id = str(i).zfill(3)\n",
    "        with open(f\"{save_dir}/batch_{batch_id}_x.npy\", \"wb\") as f_x:\n",
    "            np.save(f_x, x)\n",
    "        with open(f\"{save_dir}/batch_{batch_id}_y.npy\", \"wb\") as f_y:\n",
    "            np.save(f_y, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_generate_numpy_dataset(t=0.5):\n",
    "    df = get_images_and_emotions_df(t)\n",
    "    print(\"Number of images:\", df.shape)\n",
    "    save_dir = f\"{ROOT_DIR}/numpy_batches_{t}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    generate_numpy_dataset(df, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: (11386, 3)\n",
      "Found 11386 non-validated image filenames.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437ff1db84f34cb8bff9a6e09aa27e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=44.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot identify image file <_io.BytesIO object at 0x7feaa02cbfc0>\n",
      "cannot identify image file <_io.BytesIO object at 0x7feb19bc6360>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stathis/.local/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:792: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot identify image file <_io.BytesIO object at 0x7feb19bc60f8>\n"
     ]
    }
   ],
   "source": [
    "task_generate_numpy_dataset(0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
